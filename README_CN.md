# WorkerTransformer ğŸ‘·

[English README](README.md) | [ä¸­æ–‡è¯´æ˜](README_CN.md)

> **ç®€è¿°**: ä¸€ç§ç¨€ç–æ›´æ–°çš„ Transformer æ¶æ„ã€‚åœ¨å°è§„æ¨¡å®éªŒä¸­ï¼Œä¸æ ‡å‡† Transformer ç›¸æ¯”ï¼Œå®ƒå®ç°äº† **2.5å€ - 3å€çš„è®­ç»ƒåŠ é€Ÿ**ï¼ŒåŒæ—¶å–å¾—äº† **æ›´ä½çš„ Loss**ã€‚

**çŠ¶æ€**: å®éªŒæ€§ / æ¦‚å¿µéªŒè¯é˜¶æ®µ (POC)ã€‚  
**æµ‹è¯•ç¯å¢ƒ**: TinyShakespeare (å­—ç¬¦çº§)ã€‚  
**ç›®æ ‡**: å¯»æ±‚ç¤¾åŒºå¸®åŠ©ï¼Œåœ¨æ›´å¤§è§„æ¨¡ä¸Šè¿›è¡ŒéªŒè¯ã€‚

**å…è´£å£°æ˜**: æœ¬æ¶æ„å°šæœªåœ¨å¤§è§„æ¨¡æ•°æ®é›†ï¼ˆå¦‚ WikiText-103, RedPajamaï¼‰ä¸Šè¿›è¡Œä¸¥æ ¼éªŒè¯ï¼Œä¹Ÿæœªè®­ç»ƒè‡³ Loss é¥±å’Œé˜¶æ®µã€‚ä»¥ä¸‹ç»“æœä»…åŸºäºå°è§„æ¨¡å®éªŒï¼ˆ1M tokensï¼‰ã€‚è¿‡æ‹Ÿåˆè¡Œä¸ºå’Œé•¿æœŸç¨³å®šæ€§ä»æœ‰å¾…æ¢ç´¢ã€‚

---

## æ ¸å¿ƒç†å¿µ
æ ‡å‡† Transformer å¯¹å¾…æ¯ä¸ª Token æ˜¯ä¸€è§†åŒä»çš„ï¼šæ¯ä¸ª Token éƒ½è¦è®¡ç®— Qã€Kã€Vï¼Œå¹¶ç»è¿‡ FFN æ›´æ–°ã€‚è¿™åœ¨è®¡ç®—ä¸Šéå¸¸æ˜‚è´µï¼ˆAttention æ˜¯ $O(T^2)$ï¼ŒFFN æ˜¯ $O(T)$ï¼‰ã€‚

**WorkerTransformer** å°†è§’è‰²è§£è€¦ï¼š
1.  **Workers (ç¨€ç–)**: åªæœ‰æ¯ç¬¬ $k$ ä¸ª Tokenï¼ˆä¾‹å¦‚ $k=4$ï¼‰å……å½“ "Worker"ã€‚å®ƒæ‰§è¡Œå®Œæ•´çš„ Attention å’Œ FFNï¼Œè´Ÿè´£å…¨å±€æ¨ç†ã€‚
2.  **Tokens (ç¨ å¯†)**: å¤§å¤šæ•° Token ä»…å……å½“ "Memory"ï¼ˆè®°å¿†ï¼‰ã€‚å®ƒä»¬åªæ‰§è¡Œå»‰ä»·çš„ **Depthwise Conv1d** (Token Mixer) æ¥æ•è·å±€éƒ¨è¯­æ³•ï¼Œè·³è¿‡ç¹é‡çš„ FFN/Attention æ›´æ–°ã€‚
3.  **åŸåœ°æ›´æ–° (In-place)**: æˆ‘ä»¬ä¸å¢åŠ é¢å¤–çš„ Tokenã€‚Worker æ˜¯ *åŸåœ°* æ›´æ–°çš„ï¼Œä¿æŒåºåˆ—é•¿åº¦ä¸å˜ï¼Œä¸” KV Cache å¾ˆå°ã€‚
4.  **é—¨æ§æ³¨æ„åŠ› (Gated Attention)**: å¼•å…¥äº†æœ€æ–°çš„ç ”ç©¶æˆæœ (arXiv:2505.06708) æ¥ç¨³å®šç¨€ç–æ›´æ–°çš„è®­ç»ƒã€‚

## å®éªŒç»“æœ

æˆ‘ä»¬åœ¨å®Œå…¨ç›¸åŒçš„æ¡ä»¶ä¸‹ï¼ˆå‚æ•°é‡ã€å±‚æ•°ã€ç»´åº¦ï¼‰å¯¹æ¯”äº† **Standard Transformer** å’Œ **Inplace WorkerTransformer**ã€‚

### å®éªŒ: é•¿åºåˆ— (T=1024, é™æ—¶ 300ç§’)
*è®¾ç½®: Dim=256, Layers=4, Interval=4*

| æ¨¡å‹ | é€Ÿåº¦ (steps/s) | è®­ç»ƒæ—¶é•¿ | æœ€ç»ˆ Val Loss | 120ç§’æ—¶çš„ Loss | å‚æ•°é‡ | åŠ é€Ÿæ¯” |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| Standard Transformer | 3.22 | 300s | 1.4448 | 1.87 | 3.44M | 1.00x |
| **WorkerTransformer** | **8.02** | 300s | **1.3019** | **1.50** | 3.45M | **2.49x** |

**å…³é”®å‘ç°**: åœ¨å›ºå®šçš„ 5 åˆ†é’Ÿè®­ç»ƒä¸­ï¼Œ**WorkerTransformer è¾¾åˆ°äº† 1.30 çš„éªŒè¯é›† Lossï¼Œè€Œ Standard Transformer ä»…è¾¾åˆ° 1.44ã€‚** WorkerTransformer ä¸ä»…å•æ­¥é€Ÿåº¦å¿«ï¼Œè€Œä¸”åœ¨çœŸå®æ—¶é—´å†…çš„å­¦ä¹ æ•ˆç‡æ›´é«˜ã€‚

## å®‰è£…ä¸ä½¿ç”¨

### 1. ä¾èµ–è¦æ±‚

æœ¬é¡¹ç›®æ˜¯ **çº¯ PyTorch (Pure PyTorch)** å®ç°ã€‚ä¸éœ€è¦ç¼–è¯‘ä»»ä½•è‡ªå®šä¹‰ CUDA æ ¸å‡½æ•° (Triton/CUDA)ï¼Œè¿™ä½¿å¾—å®ƒéå¸¸å®¹æ˜“ä¿®æ”¹å’Œéƒ¨ç½²ã€‚

```bash
# åŸºç¡€ä¾èµ–
pip install torch
```

*æ³¨æ„: å¦‚éœ€ GPU åŠ é€Ÿï¼Œè¯·å®‰è£…ä¸æ‚¨çš„ CUDA ç‰ˆæœ¬å…¼å®¹çš„ PyTorch ç‰ˆæœ¬ (è¯¦è§ [pytorch.org](https://pytorch.org/get-started/locally/))ã€‚*

### 2. è¿è¡ŒåŸºå‡†æµ‹è¯•

é¦–å…ˆï¼Œä¸‹è½½ `input.txt` (TinyShakespeare) æ•°æ®é›†åˆ°æ ¹ç›®å½•ï¼š

```bash
# Linux / MacOS
wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt

# Windows (PowerShell)
Invoke-WebRequest -Uri https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -OutFile input.txt
```

è¿è¡Œä»¥ä¸‹è„šæœ¬å³å¯å¤ç°ä¸Šè¿°ç»“æœï¼š

```bash
python benchmark.py
```

### 3. åœ¨ä»£ç ä¸­ä½¿ç”¨

```python
import torch
from model import InplaceWorkerTransformer

# åˆå§‹åŒ–æ¨¡å‹
model = InplaceWorkerTransformer(
    vocab_size=1000,
    block_size=1024,
    dim=256,
    num_heads=4,
    num_layers=4,
    worker_interval=4  # æ¯ 4 ä¸ª token è®¾ä¸ºä¸€ä¸ª worker
)

# ç§»åŠ¨åˆ° GPU
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = model.to(device)

# æ„é€ å‡æ•°æ®
input_ids = torch.randint(0, 1000, (1, 1024)).to(device)
targets = torch.randint(0, 1000, (1, 1024)).to(device)

# å‰å‘ä¼ æ’­
logits, loss = model(input_ids, targets)
print(f"Loss: {loss.item()}")
```

## æ–‡ä»¶ç»“æ„
*   `model.py`: **æ ¸å¿ƒä»£ç **ã€‚åŒ…å«ä¼˜åŒ–åçš„ Inplace Worker æ¶æ„ (Token Mixing + Gated Attention)ã€‚
*   `baseline.py`: æ ‡å‡† Transformer åŸºçº¿ (ä¹ŸåŠ å…¥äº† Gated Attention ä»¥ç¡®ä¿å…¬å¹³å¯¹æ¯”)ã€‚
*   `benchmark.py`: åŸºäºæ­¥æ•°çš„åŸºå‡†æµ‹è¯•è„šæœ¬ï¼ˆå¯¹æ¯”æ¯æ­¥çš„é€Ÿåº¦å’ŒLossï¼‰ã€‚
*   `benchmark.log`: `benchmark.py` çš„è¿è¡Œæ—¥å¿—ã€‚
*   `benchmark_time.py`: åŸºäºæ—¶é—´çš„åŸºå‡†æµ‹è¯•è„šæœ¬ï¼ˆå¯¹æ¯”å›ºå®šæ—¶é—´é¢„ç®—å†…çš„æ”¶æ•›é€Ÿåº¦ï¼‰ã€‚
*   `benchmark_time.log`: `benchmark_time.py` çš„è¿è¡Œæ—¥å¿—ã€‚

## å¼•ç”¨ / è‡´è°¢
æˆ‘æ˜¯ä¸€åç‹¬ç«‹ç ”ç©¶å‘˜ï¼Œè®¡ç®—èµ„æºæœ‰é™ã€‚ç›®å‰ä»…åœ¨ `tiny_shakespeare` ä¸Šè¿›è¡Œäº†éªŒè¯ã€‚

**å¦‚æœæ‚¨è§‰å¾—è¿™ä¸ªæ¶æ„æœ‰ç”¨ï¼Œå¹¶åœ¨æ›´å¤§è§„æ¨¡ä¸Šè¿›è¡Œäº†éªŒè¯æˆ–å‘è¡¨äº†è®ºæ–‡ï¼Œè¯·å¥½å¿ƒå¼•ç”¨æœ¬ä»“åº“æˆ–é“¾æ¥å›è¿™é‡Œã€‚**

è®©æˆ‘ä»¬ä¸€èµ·è®© Transformer å†æ¬¡é«˜æ•ˆï¼

---

**æ³¨**: æœ¬ä»£ç åº“æ˜¯ä»ä¸€ä¸ªæ›´å¤§çš„å®éªŒæ€§å®éªŒå®¤ç¯å¢ƒä¸­åˆ†ç¦»å‡ºæ¥çš„ã€‚ä¸ºäº†ç¡®ä¿åœ¨ä¸åŒç¯å¢ƒä¸‹èƒ½ç‹¬ç«‹è¿è¡Œï¼Œéƒ¨åˆ†ä»£ç ç»è¿‡äº† AI çš„é€‚é…å’Œä¿®æ­£ã€‚
